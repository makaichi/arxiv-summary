import datetime
import json
import logging
import os
import sys

import arxiv
import openai
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv


class ArxivSummarizer:
    """
    A class to automatically summarize arXiv papers using LLMs.
    """

    def __init__(self):
        """
        Initializes the ArxivSummarizer class.
        Loads environment variables, sets up logging, and initializes the OpenAI client.
        """
        self._setup_logging()  # Initialize logging
        self._load_environment_variables()
        self.client = openai.OpenAI(
            api_key=self.openai_api_key, 
            base_url=self.openai_base_url,
            max_retries=5, # Enable retries
        )

    def _setup_logging(self):
        """Configures logging."""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[logging.StreamHandler(sys.stdout)],  # Output to console
        )

    def _load_environment_variables(self):
        """Loads environment variables from .env file."""
        load_dotenv()  # Load .env if it exists

        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_base_url = os.getenv("OPENAI_BASE_URL")
        self.openai_model_name = os.getenv("OPENAI_MODEL_NAME", "gpt-3.5-turbo")  # Default model
        self.summary_language = os.getenv("SUMMARY_LANGUAGE", "English")  # Default language
        self.webhook_url = os.getenv("WEBHOOK_URL")

        if not self.openai_api_key:
            logging.error("OPENAI_API_KEY not found in environment variables.")
            raise ValueError("OPENAI_API_KEY is required.")


    def _handle_exception(self, e, message="An error occurred:"):
        """Logs and potentially re-raises an exception."""
        logging.exception(f"{message} {e}")
        raise  # Re-raise to stop execution (or handle differently if needed)

    def get_paper_links_from_arxiv_page(self, url: str) -> list[str]:
        """
        Fetches all paper links (starting with /abs/) from an arXiv page.
        """
        logging.info(f"Fetching paper links from: {url}")
        try:
            response = requests.get(url)
            response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
            soup = BeautifulSoup(response.content, "html.parser")
            links = [
                a["href"] for a in soup.find_all("a", href=True) if a["href"].startswith("/abs/")
            ]
            logging.info(f"Found {len(links)} raw links.")
            return links
        except requests.exceptions.RequestException as e:
            logging.error(f"Network error fetching arXiv page {url}: {e}")
            raise  # Re-raise the exception to be handled upstream
        except Exception as e:
            # Use _handle_exception for consistency if it should halt execution
            self._handle_exception(e, "Error parsing arXiv page:") 

    def get_paper_metadata(self, paper_id: str) -> dict:
        """
        Retrieves paper metadata (title, abstract) from arXiv using the arxiv library.
        The arxiv library typically fetches the latest version if a base ID is provided.
        """
        try:
            client = arxiv.Client()
            search = arxiv.Search(id_list=[paper_id])
            results = client.results(search)
            paper = next(results)  # Get the first result
            authors = [author.name for author in paper.authors]
            # Limit authors to avoid overly long strings
            if len(authors) > 3:
                authors = authors[:2] + ["et al."]
            return {
                "title": paper.title,
                "authors": ", ".join(authors),
                "abstract": paper.summary,
                "url": paper.entry_id.replace("http://", "https://"), # Ensure HTTPS
            }

        except Exception as e:
            # Log and re-raise, but allow process_arxiv_url to catch and continue
            logging.error(f"Error fetching metadata for paper ID {paper_id}: {e}")
            raise

    def summarize_paper(self, title: str, abstract: str) -> tuple[str, str]:
        """
        Summarizes the paper and translates its title using the OpenAI API.
        Returns (translated_title, summary).
        """
        try:
            # Summarization
            summary_prompt = f"""Summarize the following research paper. Provide the most important information in up to 3 sentences. Respond in {self.summary_language}.

            Title: {title}
            Abstract: {abstract}
            """
            summary_completion = self.client.chat.completions.create(
                model=self.openai_model_name,
                messages=[{"role": "user", "content": summary_prompt}],
                temperature=0.7 # Can be a bit creative for summary
            )
            summary = summary_completion.choices[0].message.content.strip()

            # Title Translation
            title_prompt = f"Translate the following title of article to {self.summary_language}, only respond with the translated title: {title}"
            title_completion = self.client.chat.completions.create(
                model=self.openai_model_name,
                messages=[{"role": "user", "content": title_prompt}],
                temperature=0.0 # Strict translation
            )
            translated_title = title_completion.choices[0].message.content.strip()

            return translated_title, summary

        except openai.APIConnectionError as e:
            logging.error(f"Failed to connect to OpenAI API for summarization/translation: {e}")
            raise
        except openai.RateLimitError as e:
            logging.error(f"OpenAI API rate limit exceeded for summarization/translation: {e}")
            raise
        except Exception as e:
            # Log and re-raise, but allow process_arxiv_url to catch and continue
            logging.error(f"Error during summarization or translation for paper '{title}': {e}")
            raise

    def evaluate_relevance(self, title: str, abstract: str, user_interest: str) -> int:
        """
        Evaluates the relevance of a paper to the user's interest using the OpenAI API.
        Returns 0 (low), 1 (medium), or 2 (high).
        """
        prompt = f"""Given the following research paper's title and abstract, and a (list of) user's area of interest,
        rate the relevance of the paper to the user's interest.
        Respond with only a single integer:
        0 for Low relevance to all of the user's interests,
        1 for Medium relevance to any of the user's interests,
        2 for High relevance to any of the user's interests.

        User's Interest: {user_interest}

        Paper Title: {title}
        Paper Abstract: {abstract}

        Relevance Score (0, 1, or 2):"""

        try:
            completion = self.client.chat.completions.create(
                model=self.openai_model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0, # Make it deterministic for score
                max_tokens=1 # We only expect a single digit
            )
            score_str = completion.choices[0].message.content.strip()
            try:
                score = int(score_str)
                if score not in [0, 1, 2]:
                    logging.warning(f"LLM returned an unexpected relevance score: '{score_str}'. Defaulting to 0 for paper '{title}'.")
                    return 0
                return score
            except ValueError:
                logging.warning(f"Could not parse relevance score (not an integer) from LLM for paper '{title}'. Response was: '{score_str}'. Defaulting to 0.")
                return 0
        except openai.APIConnectionError as e:
            logging.error(f"Failed to connect to OpenAI API for relevance check: {e}")
            raise # Re-raise critical API errors
        except openai.RateLimitError as e:
            logging.error(f"OpenAI API rate limit exceeded for relevance check: {e}")
            raise # Re-raise critical API errors
        except Exception as e: # Catch any other unexpected errors from OpenAI client
            logging.error(f"An unexpected error occurred during relevance evaluation for paper '{title}': {e}")
            return 0 # Default to low relevance on general error


    def process_arxiv_url(self, category: str, user_interest: str | None = None, filter_level: str = "none") -> list[dict] | None:
        """
        Main function to orchestrate the process of fetching, summarizing, and evaluating papers.
        Returns a list of processed paper metadata dictionaries.
        """
        arxiv_url = f"https://arxiv.org/list/{category}/new"
        papers = []
        
        # Define relevance score mapping for filtering
        relevance_thresholds = {"low": 0, "mid": 1, "high": 2, "none": -1} # -1 means no filtering
        if not user_interest and filter_level != "none":
            logging.warning(f"User interest not specified, but filter level '{filter_level}' is set. Skipping filtering.")
            filter_level = "none"
        min_relevance_score = relevance_thresholds.get(filter_level.lower(), -1)

        try:
            paper_links = self.get_paper_links_from_arxiv_page(arxiv_url)

            for link in paper_links:
                paper_id = link.split("/")[-1]
                logging.info(f"Processing paper ID: {paper_id}")
                try:
                    metadata = self.get_paper_metadata(paper_id)
                    title = metadata["title"]
                    abstract = metadata["abstract"]
                    
                    relevance_score = 0 # Default to 0
                    if user_interest:
                        relevance_score = self.evaluate_relevance(title, abstract, user_interest)
                        logging.info(f"Relevance for '{title}': {relevance_score}")
                    
                    metadata["relevance"] = relevance_score

                    # Apply filtering based on relevance_score and filter_level
                    if min_relevance_score != -1 and relevance_score < min_relevance_score:
                        logging.info(f"Paper '{title}' (ID: {paper_id}) has relevance {relevance_score}, which is below filter level '{filter_level}' ({min_relevance_score}). Skipping summarization.")
                        continue # Skip to the next paper

                    translated_title, summary = self.summarize_paper(title, abstract)
                    metadata["summary"] = summary
                    metadata["translated_title"] = translated_title

                    if user_interest:
                        relevance_score = self.evaluate_relevance(title, abstract, user_interest)
                        metadata["relevance"] = relevance_score
                        logging.info(f"Relevance for '{title}': {relevance_score}")
                    else:
                        metadata["relevance"] = 0 # Default to 0 if no interest specified

                    papers.append(metadata)
                except (openai.APIConnectionError, openai.RateLimitError) as e:
                    # Log a warning and skip to the next paper if retries fail.
                    logging.warning(f"OpenAI API error for paper ID {paper_id} after retries: {e}. Skipping paper.")
                    continue
                except Exception as e:
                    # For other errors (e.g., arxiv library, parsing), just log and continue to the next paper
                    logging.error(f"Failed to process paper ID {paper_id}. Error: {e}")

            if not papers:
                logging.warning("No papers were successfully processed.")
                return None # Indicate no papers were processed

            # Sorting is now handled in the run method.

            return papers

        except requests.exceptions.RequestException as e:
            logging.error(f"Network error fetching arXiv page or during initial processing: {e}")
            return None
        except Exception as e:
            logging.error(f"An unhandled error occurred during overall paper processing: {e}")
            return None

    def send_arxiv_data_via_webhook(self, data_list: list, category_with_suffix: str):
        """
        Sends Arxiv paper data in a single message to a webhook.

        Args:
            data_list: A list of dictionaries, where each dictionary represents an Arxiv paper
                    and contains keys like 'title', 'url', 'authors', 'abstract', 'summary'.
            category_with_suffix: The Arxiv category string, potentially with a batch suffix.

        Returns:
            The response object from the webhook for a successful send, or None if there's an error.
        """
        if not self.webhook_url:
            logging.warning("WEBHOOK_URL not set. Skipping sending data via webhook.")
            return None

        try:
            # Construct the message content by concatenating information from all papers.
            today = datetime.datetime.now().strftime("%Y-%m-%d")
            message_text = f"{today} Arxiv papers summary for {category_with_suffix}:\n\n"
            for paper_data in data_list:
                message_text += f"Title: {paper_data['title']}\n"
                message_text += f"{paper_data['translated_title']}\n"
                message_text += f"Authors: {paper_data['authors']}\n"
                message_text += f"URL: {paper_data['url']}\n"
                if "relevance" in paper_data: # Add relevance if it exists
                    relevance_map = {0: "Low", 1: "Medium", 2: "High"}
                    message_text += f"Relevance: {relevance_map.get(paper_data['relevance'], 'N/A')}\n"
                message_text += f"Summary: {paper_data['summary']}\n\n"
            # Remove the trailing newline characters
            message_text = message_text.rstrip("\n")

            # Construct the message payload.
            content = {"text": message_text}

            payload = {"msg_type": "text", "content": content}

            # Convert the payload to JSON.
            json_payload = json.dumps(payload)

            # Send the request to the webhook.
            headers = {"Content-Type": "application/json"}
            logging.info(f"Sending {len(data_list)} papers to webhook for category {category_with_suffix}...")
            response = requests.post(self.webhook_url, data=json_payload, headers=headers)

            # Check the response status code.
            if response.status_code == 200:
                logging.info(f"Successfully sent data for {len(data_list)} papers in batch '{category_with_suffix}'.")
                return response
            else:
                logging.error(
                    f"Error sending data for batch '{category_with_suffix}'. Status code: {response.status_code}. Response text: {response.text}"
                )
                return None

        except Exception as e:
            logging.error(f"An error occurred while sending webhook for batch '{category_with_suffix}': {e}")
            return None

    def run(self, category: str, max_papers_split: int = 10, user_interest: str | None = None, filter_level: str = "none"):
        logging.info(f"Starting Arxiv summarization for category: {category}")
        papers = self.process_arxiv_url(category, user_interest, filter_level)
        if not papers:
            logging.warning("Processing failed or no papers were found. Exiting.")
            return # Exit gracefully if no papers or error during processing
        
        # Sort papers by relevance if user_interest is set
        if user_interest:
            papers.sort(key=lambda x: x.get("relevance", 0), reverse=True)
            logging.info(f"Sorted {len(papers)} papers by relevance (descending).")

        if self.webhook_url:
            num_splits = (len(papers) + max_papers_split - 1) // max_papers_split
            split_size = (len(papers) + num_splits - 1) // num_splits
            papers_split = [papers[i : i + split_size] for i in range(0, len(papers), split_size)]
            for i, papers in enumerate(papers_split):
                if len(papers_split) == 1:
                    suffix = ""
                else:
                    suffix = f" ({i+1}/{len(papers_split)})"

                self.send_arxiv_data_via_webhook(papers, category + suffix)
        else:
            logging.info("Webhook URL not configured. Papers will not be sent.")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Summarize Arxiv papers.")
    parser.add_argument(
        "category", nargs="?", default="eess.AS", help="Arxiv category (default: eess.AS)"
    )
    parser.add_argument(
        "--max_papers_split",
        type=int,
        default=10,
        help="Maximum number of papers to send in a single webhook request (default: 10)",
    )
    parser.add_argument(
        "--user_interest",
        type=str,
        default=None,
        help="User's area of interest for relevance evaluation (e.g., 'machine learning, NLP'). If not provided, all papers will have relevance 0.",
    )
    parser.add_argument(
        "--filter_level",
        type=str,
        default="none",
        choices=["low", "mid", "high", "none"],
        help="Filter papers based on relevance: 'low' (score >=0), 'mid' (score >=1), 'high' (score >=2), 'none' (no filtering). Default: 'none'.",
    )

    args = parser.parse_args()

    try:
        summarizer = ArxivSummarizer()
        summarizer.run(args.category, args.max_papers_split, args.user_interest, args.filter_level)
    except ValueError as e:
        logging.critical(f"Configuration error: {e}. Please check your .env file.")
    except Exception as e:
        logging.critical(f"An unrecoverable error occurred during execution: {e}", exc_info=True)

